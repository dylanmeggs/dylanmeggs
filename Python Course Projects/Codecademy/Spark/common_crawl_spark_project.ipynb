{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31362fa",
   "metadata": {},
   "source": [
    "# PySpark and Big Data Project -  Solutions\n",
    "* [View Solution Notebook](./solution.html)\n",
    "\n",
    "* [Project Page Link](https://www.codecademy.com/courses/big-data-pyspark/projects/pyspark-common-crawl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f30bf",
   "metadata": {},
   "source": [
    "## Task Group 1 - Analyzing Common Crawl Data with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c96e6",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Initialize a new Spark Context and read in the domain graph as an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8d3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ba7d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['367855\\t172-in-addr\\tarpa\\t1',\n",
       " '367856\\taddr\\tarpa\\t1',\n",
       " '367857\\tamphic\\tarpa\\t1',\n",
       " '367858\\tbeta\\tarpa\\t1',\n",
       " '367859\\tcallic\\tarpa\\t1',\n",
       " '367860\\tch\\tarpa\\t1',\n",
       " '367861\\td\\tarpa\\t1',\n",
       " '367862\\thome\\tarpa\\t7',\n",
       " '367863\\tiana\\tarpa\\t1',\n",
       " '367907\\tlocal\\tarpa\\t1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Domains CSV File into an RDD\n",
    "common_crawl_domain_counts = sc.textFile('./crawl/cc-main-limited-domains.csv')\n",
    "\n",
    "# Display first few domains from the RDD\n",
    "common_crawl_domain_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18a601",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Apply `fmt_domain_graph_entry` over `common_crawl_domain_counts` and save the result as a new RDD named `formatted_host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7950d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_domain_graph_entry(entry):\n",
    "    \"\"\"\n",
    "    Formats a Common Crawl domain graph entry. Extracts the site_id, \n",
    "    top-level domain (tld), domain name, and subdomain count as seperate items.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')        \n",
    "    return int(site_id), domain, tld, int(num_subdomains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f9ed9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(367855, '172-in-addr', 'arpa', 1),\n",
       " (367856, 'addr', 'arpa', 1),\n",
       " (367857, 'amphic', 'arpa', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply `fmt_domain_graph_entry` to the raw data RDD\n",
    "formatted_host_counts = common_crawl_domain_counts.map(fmt_domain_graph_entry)\n",
    "\n",
    "# Display the first few entries of the new RDD\n",
    "formatted_host_counts.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a9aae",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Apply `extract_subdomain_counts` over `common_crawl_domain_counts` and save the result as a new RDD named `host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfb75dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 7, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_subdomain_counts(entry):\n",
    "    \"\"\"\n",
    "    Extract the subdomain count from a Common Crawl domain graph entry.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')\n",
    "    \n",
    "    # return ONLY the num_subdomains\n",
    "    return int(num_subdomains)\n",
    "\n",
    "\n",
    "# Apply `extract_subdomain_counts` to the raw data RDD\n",
    "host_counts = common_crawl_domain_counts.map(extract_subdomain_counts)\n",
    "\n",
    "# Display the first few entries\n",
    "host_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44483f3",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Using `host_counts`, calculate the total number of subdomains across all domains in the dataset, save the result to a variable named `total_host_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa284001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595466\n"
     ]
    }
   ],
   "source": [
    "# Reduce the RDD to a single value, the sum of subdomains, with a lambda function\n",
    "# as the reduce function\n",
    "total_host_counts = host_counts.reduce(lambda x,y: (x+y))\n",
    "\n",
    "# Display result count\n",
    "print(total_host_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5579e",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Stop the current `SparkSession` and `sparkContext` before moving on to analyze the data with SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "562745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the sparkContext and the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129687d",
   "metadata": {},
   "source": [
    "## Task Group 2 - Exploring Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22e2b6",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Create a new `SparkSession` and assign it to a variable named `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99565365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .config('spark.app.name','domain_counts_project')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f3a0c",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Read `./crawl/cc-main-limited-domains.csv` into a new Spark DataFrame named `common_crawl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "297084db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----+---+\n",
      "|_c0     |_c1           |_c2 |_c3|\n",
      "+--------+--------------+----+---+\n",
      "|367855  |172-in-addr   |arpa|1  |\n",
      "|367856  |addr          |arpa|1  |\n",
      "|367857  |amphic        |arpa|1  |\n",
      "|367858  |beta          |arpa|1  |\n",
      "|367859  |callic        |arpa|1  |\n",
      "|367860  |ch            |arpa|1  |\n",
      "|367861  |d             |arpa|1  |\n",
      "|367862  |home          |arpa|7  |\n",
      "|367863  |iana          |arpa|1  |\n",
      "|367907  |local         |arpa|1  |\n",
      "|367908  |nic           |arpa|1  |\n",
      "|48987160|1-20media     |coop|1  |\n",
      "|48987161|134           |coop|1  |\n",
      "|48987162|19            |coop|4  |\n",
      "|48987163|1strochdale   |coop|1  |\n",
      "|48987164|2012          |coop|4  |\n",
      "|48987165|2012intlsummit|coop|1  |\n",
      "|48987166|2147mans      |coop|1  |\n",
      "|48987167|21stcentury   |coop|1  |\n",
      "|48987168|2decologico   |coop|1  |\n",
      "+--------+--------------+----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- top_level_domain: string (nullable = true)\n",
      " |-- num_subdomains: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the target file into a DataFrame\n",
    "common_crawl = spark.read\\\n",
    "    .option('header', False) \\\n",
    "    .option('delimiter', '\\t') \\\n",
    "    .option('inferSchema', False) \\\n",
    "    .csv(\"./crawl/cc-main-limited-domains.csv\")\n",
    "\n",
    "# Alternative if your file has no headers in it:\n",
    "# Convert csv to RDD: rdd = spark.sparkContext.textFile(\"./crawl/cc-main-limited-domains.csv\")\n",
    "# Convert RDD to DF: rdd.toDF(['column_1', 'column_2',...])\n",
    "\n",
    "\n",
    "# Display the DataFrame to the notebook\n",
    "common_crawl.show(truncate=False)\n",
    "\n",
    "\n",
    "common_crawl_domains.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23082fd2",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Rename the DataFrame's columns to the following: \n",
    "\n",
    "- site_id\n",
    "- domain\n",
    "- top_level_domain\n",
    "- num_subdomains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f7b4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------------+--------------+\n",
      "|site_id |domain        |top_level_domain|num_subdomains|\n",
      "+--------+--------------+----------------+--------------+\n",
      "|367855  |172-in-addr   |arpa            |1             |\n",
      "|367856  |addr          |arpa            |1             |\n",
      "|367857  |amphic        |arpa            |1             |\n",
      "|367858  |beta          |arpa            |1             |\n",
      "|367859  |callic        |arpa            |1             |\n",
      "|367860  |ch            |arpa            |1             |\n",
      "|367861  |d             |arpa            |1             |\n",
      "|367862  |home          |arpa            |7             |\n",
      "|367863  |iana          |arpa            |1             |\n",
      "|367907  |local         |arpa            |1             |\n",
      "|367908  |nic           |arpa            |1             |\n",
      "|48987160|1-20media     |coop            |1             |\n",
      "|48987161|134           |coop            |1             |\n",
      "|48987162|19            |coop            |4             |\n",
      "|48987163|1strochdale   |coop            |1             |\n",
      "|48987164|2012          |coop            |4             |\n",
      "|48987165|2012intlsummit|coop            |1             |\n",
      "|48987166|2147mans      |coop            |1             |\n",
      "|48987167|21stcentury   |coop            |1             |\n",
      "|48987168|2decologico   |coop            |1             |\n",
      "+--------+--------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- top_level_domain: string (nullable = true)\n",
      " |-- num_subdomains: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the DataFrame's columns with `withColumnRenamed()`\n",
    "common_crawl = common_crawl.toDF('site_id','domain','top_level_domain','num_subdomains')\n",
    "\n",
    "\n",
    "  \n",
    "# Display the first few rows of the DataFrame and the new schema\n",
    "common_crawl.show(20,truncate=False)\n",
    "\n",
    "common_crawl_domains.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff524e08",
   "metadata": {},
   "source": [
    "## Task Group 3 - Reading and Writing Datasets to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00bc518",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "Save the `common_crawl` DataFrame as parquet files in a directory called `./results/common_crawl/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33be3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the `common_crawl` DataFrame to a series of parquet files\n",
    "common_crawl.write.parquet('./results/common_crawl/',mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbf8ff",
   "metadata": {},
   "source": [
    "### Task 10\n",
    "\n",
    "Read `./results/common_crawl/` into a new DataFrame to confirm our DataFrame was saved properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a99ceb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------------+--------------+\n",
      "|site_id|domain     |top_level_domain|num_subdomains|\n",
      "+-------+-----------+----------------+--------------+\n",
      "|367855 |172-in-addr|arpa            |1             |\n",
      "|367856 |addr       |arpa            |1             |\n",
      "|367857 |amphic     |arpa            |1             |\n",
      "|367858 |beta       |arpa            |1             |\n",
      "|367859 |callic     |arpa            |1             |\n",
      "|367860 |ch         |arpa            |1             |\n",
      "|367861 |d          |arpa            |1             |\n",
      "|367862 |home       |arpa            |7             |\n",
      "|367863 |iana       |arpa            |1             |\n",
      "|367907 |local      |arpa            |1             |\n",
      "+-------+-----------+----------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- top_level_domain: string (nullable = true)\n",
      " |-- num_subdomains: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from parquet directory\n",
    "common_crawl_domains = spark.read\\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', '\\t') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .parquet(\"./results/common_crawl/\")\n",
    "\n",
    "# Display the first few rows of the DataFrame and the schema\n",
    "common_crawl_domains.show(10,truncate=False)\n",
    "\n",
    "common_crawl_domains.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f34ede",
   "metadata": {},
   "source": [
    "## Task Group 4 - Querying Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71895cf",
   "metadata": {},
   "source": [
    "### Task 11\n",
    "\n",
    "Create a local temporary view from `common_crawl_domains`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdd04b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view in the metadata for this `SparkSession`\n",
    "common_crawl_domains.createOrReplaceTempView(\"crawl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2ef4f",
   "metadata": {},
   "source": [
    "### Task 12\n",
    "\n",
    "Calculate the total number of domains for each top-level domain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8f79679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+\n",
      "|top_level_domain|domain_count|\n",
      "+----------------+------------+\n",
      "|edu             |18547       |\n",
      "|gov             |15007       |\n",
      "|travel          |6313        |\n",
      "|coop            |5319        |\n",
      "|jobs            |3893        |\n",
      "|post            |117         |\n",
      "|map             |34          |\n",
      "|arpa            |11          |\n",
      "+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "# Count unique domains per TLD without importing functions\n",
    "common_crawl_domains \\\n",
    ".select('top_level_domain', 'domain')\\\n",
    ".distinct()\\\n",
    ".groupBy('top_level_domain')\\\n",
    ".count()\\\n",
    ".withColumnRenamed(\"count\", \"domain_count\")\\\n",
    ".orderBy(\"domain_count\", ascending=False)\\\n",
    ".show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dae3e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+\n",
      "|top_level_domain|domain_count|\n",
      "+----------------+------------+\n",
      "|edu             |18547       |\n",
      "|gov             |15007       |\n",
      "|travel          |6313        |\n",
      "|coop            |5319        |\n",
      "|jobs            |3893        |\n",
      "|post            |117         |\n",
      "|map             |34          |\n",
      "|arpa            |11          |\n",
      "+----------------+------------+\n",
      "\n",
      "root\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- top_level_domain: string (nullable = true)\n",
      " |-- num_subdomains: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        top_level_domain,\n",
    "        COUNT(DISTINCT domain) AS domain_count\n",
    "    FROM crawl\n",
    "    GROUP BY top_level_domain\n",
    "    ORDER BY domain_count DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "#common_crawl_domains.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb11c33",
   "metadata": {},
   "source": [
    "### Task 13\n",
    "\n",
    "Calculate the total number of subdomains for each top-level domain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "502578e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+\n",
      "|top_level_domain|domain_count|\n",
      "+----------------+------------+\n",
      "|edu             |484438      |\n",
      "|gov             |85354       |\n",
      "|travel          |10768       |\n",
      "|coop            |8683        |\n",
      "|jobs            |6023        |\n",
      "|post            |143         |\n",
      "|map             |40          |\n",
      "|arpa            |17          |\n",
      "+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "common_crawl_domains \\\n",
    "    .select('top_level_domain', common_crawl_domains.num_subdomains.cast('int')) \\\n",
    "    .groupBy('top_level_domain') \\\n",
    "    .sum('num_subdomains') \\\n",
    "    .withColumnRenamed('sum(num_subdomains)', 'domain_count') \\\n",
    "    .orderBy('domain_count', ascending=False) \\\n",
    "    .show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4539ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+\n",
      "|top_level_domain|domain_count|\n",
      "+----------------+------------+\n",
      "|edu             |484438      |\n",
      "|gov             |85354       |\n",
      "|travel          |10768       |\n",
      "|coop            |8683        |\n",
      "|jobs            |6023        |\n",
      "|post            |143         |\n",
      "|map             |40          |\n",
      "|arpa            |17          |\n",
      "+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        top_level_domain,\n",
    "        sum(cast(num_subdomains AS int)) AS domain_count\n",
    "    FROM crawl\n",
    "    GROUP BY top_level_domain\n",
    "    ORDER BY domain_count DESC\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2dfea3",
   "metadata": {},
   "source": [
    "### Task 14\n",
    "\n",
    "How many sub-domains does `nps.gov` have? Filter the dataset to that website's entry, display the columns `top_level_domain`, `domain`, and `num_subdomains` in your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b45051e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+--------------+\n",
      "|top_level_domain|domain|num_subdomains|\n",
      "+----------------+------+--------------+\n",
      "|gov             |nps   |178           |\n",
      "+----------------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame using DataFrame Methods\n",
    "common_crawl_domains \\\n",
    "    .select('top_level_domain', 'domain', 'num_subdomains') \\\n",
    "    .filter(\n",
    "            (common_crawl_domains.top_level_domain == 'gov') &\n",
    "            (common_crawl_domains.domain == 'nps')\n",
    "        ) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1746a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+--------------+\n",
      "|top_level_domain|domain|num_subdomains|\n",
      "+----------------+------+--------------+\n",
      "|gov             |nps   |178           |\n",
      "+----------------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame using SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        top_level_domain,\n",
    "        domain,\n",
    "        num_subdomains\n",
    "    FROM crawl\n",
    "    where domain = 'nps'\n",
    "    and top_level_domain = 'gov'\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336d454",
   "metadata": {},
   "source": [
    "### Task 15\n",
    "\n",
    "Close the `SparkSession` and underlying `sparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2233037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the notebook's `SparkSession` and `sparkContext`\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc037a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
